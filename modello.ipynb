{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_random_seed' from 'tensorflow' (c:\\Users\\marco\\miniconda3\\Lib\\site-packages\\tensorflow\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed\n\u001b[0;32m     20\u001b[0m seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n\u001b[0;32m     22\u001b[0m set_random_seed(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'set_random_seed' from 'tensorflow' (c:\\Users\\marco\\miniconda3\\Lib\\site-packages\\tensorflow\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv('../input/artists.csv')\n",
    "artists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort artists by number of paintings\n",
    "artists = artists.sort_values(by=['paintings'], ascending=False)\n",
    "\n",
    "# Create a dataframe with artists having more than 200 paintings\n",
    "artists_top = artists[artists['paintings'] >= 200].reset_index()\n",
    "artists_top = artists_top[['name', 'paintings']]\n",
    "#artists_top['class_weight'] = max(artists_top.paintings)/artists_top.paintings\n",
    "artists_top['class_weight'] = artists_top.paintings.sum() / (artists_top.shape[0] * artists_top.paintings)\n",
    "artists_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set class weights - assign higher weights to underrepresented classes\n",
    "class_weights = artists_top['class_weight'].to_dict()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is some problem recognizing 'Albrecht_Dürer' (don't know why, worth exploring)\n",
    "# So I'll update this string as directory name to df's\n",
    "updated_name = \"Albrecht_Dürer\".replace(\"_\", \" \")\n",
    "artists_top.iloc[4, 0] = updated_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore images of top artists\n",
    "images_dir = '../input/images/images'\n",
    "artists_dirs = os.listdir(images_dir)\n",
    "artists_top_name = artists_top['name'].str.replace(' ', '_').values\n",
    "\n",
    "# See if all directories exist\n",
    "for name in artists_top_name:\n",
    "    if os.path.exists(os.path.join(images_dir, name)):\n",
    "        print(\"Found -->\", os.path.join(images_dir, name))\n",
    "    else:\n",
    "        print(\"Did not find -->\", os.path.join(images_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Print few random paintings\n",
    "n = 5\n",
    "fig, axes = plt.subplots(1, n, figsize=(20,10))\n",
    "\n",
    "for i in range(n):\n",
    "    random_artist = random.choice(artists_top_name)\n",
    "    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n",
    "    random_image_file = os.path.join(images_dir, random_artist, random_image)\n",
    "    image = plt.imread(random_image_file)\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(\"Artist: \" + random_artist.replace('_', ' '))\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data\n",
    "batch_size = 16\n",
    "train_input_shape = (224, 224, 3)\n",
    "n_classes = artists_top.shape[0]\n",
    "\n",
    "train_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                   rescale=1./255.,\n",
    "                                   #rotation_range=45,\n",
    "                                   #width_shift_range=0.5,\n",
    "                                   #height_shift_range=0.5,\n",
    "                                   shear_range=5,\n",
    "                                   #zoom_range=0.7,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                  )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory=images_dir,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=train_input_shape[0:2],\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset=\"training\",\n",
    "                                                    shuffle=True,\n",
    "                                                    classes=artists_top_name.tolist()\n",
    "                                                   )\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(directory=images_dir,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=train_input_shape[0:2],\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset=\"validation\",\n",
    "                                                    shuffle=True,\n",
    "                                                    classes=artists_top_name.tolist()\n",
    "                                                   )\n",
    "\n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "print(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a random paintings and it's random augmented version\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "random_artist = random.choice(artists_top_name)\n",
    "random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n",
    "random_image_file = os.path.join(images_dir, random_artist, random_image)\n",
    "\n",
    "# Original image\n",
    "image = plt.imread(random_image_file)\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"An original Image of \" + random_artist.replace('_', ' '))\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Transformed image\n",
    "aug_image = train_datagen.random_transform(image)\n",
    "axes[1].imshow(aug_image)\n",
    "axes[1].set_title(\"A transformed Image of \" + random_artist.replace('_', ' '))\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers at the end\n",
    "X = base_model.output\n",
    "X = Flatten()(X)\n",
    "\n",
    "X = Dense(512, kernel_initializer='he_uniform')(X)\n",
    "#X = Dropout(0.5)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "X = Dense(16, kernel_initializer='he_uniform')(X)\n",
    "#X = Dropout(0.5)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "output = Dense(n_classes, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n",
    "                           mode='auto', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n",
    "                              verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model - all layers\n",
    "history1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=n_epoch,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[reduce_lr],\n",
    "                              use_multiprocessing=True,\n",
    "                              workers=16,\n",
    "                              class_weight=class_weights\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze core ResNet layers and train again \n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[:50]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "n_epoch = 50\n",
    "history2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=n_epoch,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[reduce_lr, early_stop],\n",
    "                              use_multiprocessing=True,\n",
    "                              workers=16,\n",
    "                              class_weight=class_weights\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge history1 and history2\n",
    "history = {}\n",
    "history['loss'] = history1.history['loss'] + history2.history['loss']\n",
    "history['acc'] = history1.history['acc'] + history2.history['acc']\n",
    "history['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\n",
    "history['val_acc'] = history1.history['val_acc'] + history2.history['val_acc']\n",
    "history['lr'] = history1.history['lr'] + history2.history['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training graph\n",
    "def plot_training(history):\n",
    "    acc = history['acc']\n",
    "    val_acc = history['val_acc']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n",
    "    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n",
    "    axes[0].set_title('Training and Validation Accuracy')\n",
    "    axes[0].legend(loc='best')\n",
    "\n",
    "    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n",
    "    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n",
    "    axes[1].set_title('Training and Validation Loss')\n",
    "    axes[1].legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy on train data\n",
    "score = model.evaluate_generator(train_generator, verbose=1)\n",
    "print(\"Prediction accuracy on train data =\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy on CV data\n",
    "score = model.evaluate_generator(valid_generator, verbose=1)\n",
    "print(\"Prediction accuracy on CV data =\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report and confusion matrix\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "\n",
    "tick_labels = artists_top_name.tolist()\n",
    "\n",
    "def showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n",
    "    # Loop on each generator batch and predict\n",
    "    y_pred, y_true = [], []\n",
    "    for i in range(STEP_SIZE_VALID):\n",
    "        (X,y) = next(valid_generator)\n",
    "        y_pred.append(model.predict(X))\n",
    "        y_true.append(y)\n",
    "    \n",
    "    # Create a flat list for y_true and y_pred\n",
    "    y_pred = [subresult for result in y_pred for subresult in result]\n",
    "    y_true = [subresult for result in y_true for subresult in result]\n",
    "    \n",
    "    # Update Truth vector based on argmax\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    \n",
    "    # Update Prediction vector based on argmax\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n",
    "    conf_matrix = conf_matrix/np.sum(conf_matrix, axis=1)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n",
    "                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n",
    "                ax=ax)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artists_top_name.tolist()))\n",
    "\n",
    "showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "from keras.preprocessing import *\n",
    "\n",
    "n = 5\n",
    "fig, axes = plt.subplots(1, n, figsize=(25,10))\n",
    "\n",
    "for i in range(n):\n",
    "    random_artist = random.choice(artists_top_name)\n",
    "    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n",
    "    random_image_file = os.path.join(images_dir, random_artist, random_image)\n",
    "\n",
    "    # Original image\n",
    "\n",
    "    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))\n",
    "\n",
    "    # Predict artist\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image /= 255.\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    prediction = model.predict(test_image)\n",
    "    prediction_probability = np.amax(prediction)\n",
    "    prediction_idx = np.argmax(prediction)\n",
    "\n",
    "    labels = train_generator.class_indices\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "\n",
    "    #print(\"Actual artist =\", random_artist.replace('_', ' '))\n",
    "    #print(\"Predicted artist =\", labels[prediction_idx].replace('_', ' '))\n",
    "    #print(\"Prediction probability =\", prediction_probability*100, \"%\")\n",
    "\n",
    "    title = \"Actual artist = {}\\nPredicted artist = {}\\nPrediction probability = {:.2f} %\" \\\n",
    "                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n",
    "                        prediction_probability*100)\n",
    "\n",
    "    # Print image\n",
    "    axes[i].imshow(plt.imread(random_image_file))\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict from web - this is an image of Titian.\n",
    "# Replace 'url' with any image of one of the 11 artists above and run this cell.\n",
    "url = 'https://www.gpsmycity.com/img/gd/2081.jpg'\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "web_image = imageio.imread(url)\n",
    "web_image = cv2.resize(web_image, dsize=train_input_shape[0:2], )\n",
    "web_image = image.img_to_array(web_image)\n",
    "web_image /= 255.\n",
    "web_image = np.expand_dims(web_image, axis=0)\n",
    "\n",
    "\n",
    "prediction = model.predict(web_image)\n",
    "prediction_probability = np.amax(prediction)\n",
    "prediction_idx = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted artist =\", labels[prediction_idx].replace('_', ' '))\n",
    "print(\"Prediction probability =\", prediction_probability*100, \"%\")\n",
    "\n",
    "plt.imshow(imageio.imread(url))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
